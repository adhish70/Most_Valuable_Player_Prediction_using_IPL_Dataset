{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName('Ops').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"E:/Rutgers/Projects/MDSR/IPL-MSDR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Data\n",
    "matches = pd.read_csv(path + '/dataset/original_ipldata/matches.csv')\n",
    "deliveries = pd.read_csv(path + '/dataset/original_ipldata/deliveries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema of original data (matches.csv)\n",
    "matches.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema of original data (deliveries.csv)\n",
    "deliveries.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns that are of no use\n",
    "matches = matches.drop(columns = ['umpire1', 'umpire2','umpire3','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filing missing values\n",
    "matches['winner'].fillna('Draw', inplace=True)\n",
    "matches['city'].fillna('Dubai',inplace=True)\n",
    "deliveries = deliveries.fillna(value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing error Data\n",
    "matches = matches.replace('Rising Pune Supergiants', 'Rising Pune Supergiant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema of cleaned data (matches.csv)\n",
    "matches.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema of cleaned data (deliveries.csv)\n",
    "deliveries.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving cleaned data (matches.csv)\n",
    "matches.to_csv(path + '/dataset/clean_data/matches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving cleaned data (deliveries.csv)\n",
    "deliveries.to_csv(path + '/dataset/clean_data/deliveries.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teams playing in the league\n",
    "teams = matches['team1'].unique()\n",
    "print(\"Total number of teams participated so far: \" + str(len(matches['team1'].unique())))\n",
    "print(\"Teams participated so far: \")\n",
    "for i in teams:\n",
    "    print(\"- \" + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Venues\n",
    "print(\"Number of venues matches were played: \" + str(len(matches['venue'].unique())))\n",
    "for i in matches['venue'].unique():\n",
    "    print(\"- \" + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cities the matches were played\n",
    "print(\"Number of cities matches were played: \" + str(len(matches['city'].unique())))\n",
    "for i in matches['city'].unique():\n",
    "    print(\"- \" + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of bowlers so far\n",
    "print(\"Total number of bowlers: \" + str(len(deliveries['bowler'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of batsmen so far\n",
    "print(\"Total number of batsmen: \" + str(len(deliveries['batsman'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of participating players\n",
    "players = set()\n",
    "for i in range(len(deliveries['match_id'])):\n",
    "    players.add(deliveries['bowler'][i])\n",
    "    players.add(deliveries['batsman'][i])\n",
    "    players.add(deliveries['non_striker'][i])\n",
    "print(\"Total number of player: \" + str(len(players)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = spark.read.csv(path + '/dataset/clean_data/matches.csv',inferSchema=True,header=True)\n",
    "deliveries = spark.read.csv(path + '/dataset/clean_data/deliveries.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total number of matches per season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.registerTempTable('seasons')\n",
    "seasons = spark.sql('''Select distinct(season),count(*) as total_matches from seasons group by season ''') \n",
    "seasons.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, a = plt.subplots()\n",
    "a = sns.barplot(x =\"season\", y=\"total_matches\", data=seasons.toPandas(),palette='viridis')\n",
    "a.set_xlabel('Season')\n",
    "a.set_ylabel('Total Matches')\n",
    "a.set_title('Number of matches in each season')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of maches played by each team since season 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.registerTempTable('team')\n",
    "team = spark.sql('''Select distinct(team), count(*) as total_matches from (Select team1 as team from team UNION ALL (select team2 as team from team)) group by team ''')\n",
    "team.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, a = plt.subplots(figsize = (5,5))\n",
    "a = sns.barplot(x =\"total_matches\", y=\"team\", data=team.toPandas(), palette='viridis')\n",
    "a.set_ylabel('Team')\n",
    "a.set_xlabel('Total Matches')\n",
    "a.set_title('Number of matches played by each team')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total season in which teams have played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.registerTempTable('team_season')\n",
    "team_season = spark.sql('''Select team1 as team, min(season) as first_season, max(season) as last_season, count(distinct(season)) as total_seasons from team_season group by team1 order by total_seasons desc''')\n",
    "team_season.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total number of matches won by teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.registerTempTable('most_win')\n",
    "most_win = spark.sql('''Select distinct(winner) as team, count(*) as total_matches from most_win where winner <>'None' group by winner order by total_matches ''')\n",
    "most_win.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, a = plt.subplots(figsize = (5,5))\n",
    "a = sns.barplot(x =\"total_matches\", y=\"team\", data=most_win.toPandas(), palette='viridis')\n",
    "a.set_ylabel('Team')\n",
    "a.set_xlabel('Total Matches')\n",
    "a.set_title('Number of matches won by each team')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total matches won by teams in each season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.registerTempTable('most_win_by_season')\n",
    "most_win_by_season = spark.sql('''Select season, winner as team, count(*) as total_matches_won from most_win_by_season where winner <> 'None' group by season, winner order by total_matches_won desc''')\n",
    "most_win_by_season.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Players with maximum man of the match awards "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.registerTempTable('man_match')\n",
    "man_match = spark.sql('''Select distinct(player_of_match), count(*) as total_matches from man_match group by player_of_match order by total_matches desc limit 10 ''')\n",
    "man_match.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, a = plt.subplots(figsize = (5,5))\n",
    "a = sns.barplot(x =\"total_matches\", y=\"player_of_match\", data=man_match.toPandas(), palette='viridis')\n",
    "a.set_xlabel('Total Matches')\n",
    "a.set_ylabel('Player')\n",
    "a.set_title('Number of times player won man of the match')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of matches per Venue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.registerTempTable('venue')\n",
    "venue = spark.sql('''Select distinct(venue), count(*) as total_matches from venue group by venue''')\n",
    "venue.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, a = plt.subplots(figsize = (10,20))\n",
    "a = sns.barplot(x =\"total_matches\", y=\"venue\", data=venue.toPandas(), palette='viridis')\n",
    "a.set_ylabel('Venue')\n",
    "a.set_xlabel('Total Matches')\n",
    "a.set_title('Number of matches at each venue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage toss decisions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.registerTempTable('toss')\n",
    "toss = spark.sql('''Select distinct(toss_decision), ((count(toss_decision)*100)/ (select count(*) from toss)) as percentage_count from toss group by toss_decision''')\n",
    "toss.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, a = plt.subplots(figsize = (5,5))\n",
    "a = sns.barplot(x =\"toss_decision\", y=\"percentage_count\", data=toss.toPandas(), palette='viridis')\n",
    "a.set_ylabel('Percentage')\n",
    "a.set_xlabel('Toss Decision')\n",
    "a.set_title('Percentage Plot of toss_decision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage of team winning the toss as well as the match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.registerTempTable('toss_and_won')\n",
    "matches.registerTempTable('toss_won_data')\n",
    "toss_won_data = spark.sql('''Select t1.season, t1.total_matches, \\\n",
    "          t2.count_toss_and_won as count_toss_and_won, \\\n",
    "          (t2.count_toss_and_won / t1.total_matches * 100) as percent_toss_and_won from \\\n",
    "          (Select distinct(season),count(*) as total_matches from seasons group by season)t1 \\\n",
    "          left join (Select distinct(season), count(*) as count_toss_and_won from toss_and_won where toss_winner = winner group by season)t2 on t1.season = t2.season order by season''')\n",
    "toss_won_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, a = plt.subplots(figsize = (10,5))\n",
    "a = sns.barplot(x =\"season\", y=\"percent_toss_and_won\", data=toss_won_data.toPandas(), palette='viridis')\n",
    "a.set_ylabel('Percentage')\n",
    "a.set_xlabel('Season')\n",
    "a.set_title('Percentage Plot of Season and Toss_and_won')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage matches won by batting first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_batting_first = spark.sql('''Select t1.season, t1.total_matches, \\\n",
    "          t2.win_batting_first as win_batting_first, \\\n",
    "          (t2.win_batting_first/ t1.total_matches * 100) as percent_win_batting_first from \\\n",
    "          (Select distinct(season),count(*) as total_matches from seasons group by season)t1 \\\n",
    "          left join (Select distinct(season), count(*) as win_batting_first from seasons where win_by_runs > 0  group by season)t2 on t1.season = t2.season order by season ''')\n",
    "win_batting_first.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, a = plt.subplots(figsize = (10,5))\n",
    "a = sns.barplot(x =\"season\", y=\"percent_win_batting_first\", data=win_batting_first.toPandas(), palette='viridis')\n",
    "a.set_ylabel('Percentage')\n",
    "a.set_xlabel('Season')\n",
    "a.set_title('Percentage Plot of Season and won by batting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage matches won by fielding first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_bowling_first = spark.sql('''Select t1.season, t1.total_matches, \\\n",
    "          t2.win_bowling_first as win_bowling_first, \\\n",
    "          (t2.win_bowling_first/ t1.total_matches * 100) as percent_win_bowling_first from \\\n",
    "          (Select distinct(season),count(*) as total_matches from seasons group by season)t1 \\\n",
    "          left join (Select distinct(season), count(*) as win_bowling_first from seasons where win_by_wickets > 0  group by season)t2 on t1.season = t2.season order by season ''')\n",
    "win_bowling_first.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, a = plt.subplots(figsize = (10,5))\n",
    "a = sns.barplot(x =\"season\", y=\"percent_win_bowling_first\", data=win_bowling_first.toPandas(), palette='viridis')\n",
    "a.set_ylabel('Percentage')\n",
    "a.set_xlabel('Season')\n",
    "a.set_title('Percentage Plot of Season and won by wickets ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "matches = spark.read.csv(path + '/dataset/clean_data/matches.csv',inferSchema=True,header=True)\n",
    "deliveries = spark.read.csv(path + '/dataset/clean_data/deliveries.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating temporary tables of the data\n",
    "matches.registerTempTable('matches_db')\n",
    "deliveries.registerTempTable('deliveries_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging both the tables\n",
    "merged_db = spark.sql('select m.*,d.* from matches_db as m inner join deliveries_db as d on m.id=d.match_id')\n",
    "merged_db.registerTempTable('analysis_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batting Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nmba: no. of batsmen\n",
    "# nm: no. of matches played by a batsman\n",
    "# hha: hard hitting ability\n",
    "# f: finisher\n",
    "# fsa: fast scoring ability\n",
    "# con: consistency\n",
    "# rbw: running between wickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Batsmen\n",
    "nmba = spark.sql('select count(distinct(batsman)) as No_of_Batsman from analysis_db')\n",
    "nmba.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Matches played by a batsmen\n",
    "nm = spark.sql('select batsman, count(distinct(match_id)) as No_of_Matches \\\n",
    "                from analysis_db group by batsman')\n",
    "nm.registerTempTable('no_of_matches_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Hitting Ability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard Hitting Ability = (4*Fours + 6*Sixes)/Balls Played by Batsman\n",
    "hha = spark.sql('select nmt.batsman as Batsman, round(nvl(t4.hard_hitting_ability,0), 5) as \\\n",
    "                Hard_Hitting_Ability from \\\n",
    "                (select t1.batsman, (t1.fours*4 + t2.sixes*6)/t3.balls_played as hard_hitting_ability\\\n",
    "                from (select batsman,count(*) as fours from analysis_db where batsman_runs = 4 group by batsman) t1 \\\n",
    "                inner join  \\\n",
    "                (select batsman,count(*) as sixes from analysis_db where batsman_runs = 6 \\\n",
    "                group by batsman) t2 on t1.batsman=t2.batsman\\\n",
    "                inner join\\\n",
    "                (select batsman,count(*) as balls_played from analysis_db \\\n",
    "                group by batsman) t3 on t3.batsman=t1.batsman) t4 \\\n",
    "                right join \\\n",
    "                no_of_matches_table nmt on t4.batsman = nmt.batsman')\n",
    "hha.registerTempTable('hard_hitting_ability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hha = spark.sql('select rank() over (order by Hard_Hitting_Ability desc) as Rank, t1.* \\\n",
    "                  from hard_hitting_ability t1 \\\n",
    "                  inner join \\\n",
    "                  no_of_matches_table t2\\\n",
    "                  on t1.batsman = t2.batsman where no_of_matches>9')\n",
    "hha.registerTempTable('hard_hitting_ability')\n",
    "hha.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hha = spark.sql('select t1.*, round((240-rank)/240, 5) as Points, round((240-rank)*1.25/240, 5) as Weights \\\n",
    "                from hard_hitting_ability t1')\n",
    "hha.registerTempTable('hard_hitting_ability')\n",
    "hha.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finisher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finisher = Not Out innings/Total Innings played\n",
    "f = spark.sql('select t3.batsman as Batsman, round(t3.not_out_innings/t4.total_matches_played, 5) as Finisher from\\\n",
    "              (select t1.batsman, t1.matches_played-t2.number_of_times_out as not_out_innings from \\\n",
    "              (select batsman, count(distinct(match_id)) as matches_played from analysis_db group by batsman) t1\\\n",
    "              inner join \\\n",
    "              (select batsman, count(*) as number_of_times_out from analysis_db where player_dismissed = batsman group by batsman) t2\\\n",
    "              on t1.batsman=t2.batsman) t3\\\n",
    "              inner join\\\n",
    "              (select batsman, count(distinct(match_id)) as total_matches_played \\\n",
    "              from analysis_db group by batsman) t4\\\n",
    "              on t3.batsman = t4.batsman')\n",
    "f.registerTempTable('finisher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = spark.sql('select rank() over (order by finisher desc) as Rank, t1.* \\\n",
    "              from finisher t1 \\\n",
    "              inner join \\\n",
    "              no_of_matches_table t2 \\\n",
    "              on t1.batsman = t2.batsman \\\n",
    "              where no_of_matches>9')\n",
    "f.registerTempTable('finisher')\n",
    "f.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = spark.sql('select t1.*, round((240-rank)/240, 5) as Points, round((240-rank)*1.25/240, 5) as Weights \\\n",
    "              from finisher t1')\n",
    "f.registerTempTable('finisher')\n",
    "f.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Scoring Ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast Scoring Ability = Total Runs/Balls Played by Batsman\n",
    "fsa = spark.sql('select batsman as Batsman, round(Total_Runs/balls_played, 5) as Fast_Scoring_Ability \\\n",
    "                  from (select batsman,sum(batsman_runs) as Total_Runs, count(*) as balls_played \\\n",
    "                  from analysis_db group by batsman)')\n",
    "fsa.registerTempTable('fast_scoring_ability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsa = spark.sql('select rank() over (order by fast_scoring_ability desc) as Rank, t1.* \\\n",
    "                  from fast_scoring_ability t1 \\\n",
    "                  inner join \\\n",
    "                  no_of_matches_table t2 \\\n",
    "                  on t1.batsman = t2.batsman where no_of_matches>9')\n",
    "fsa.registerTempTable('fast_scoring_ability')\n",
    "fsa.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsa = spark.sql('select t1.*, round((240-rank)/240, 5) as Points, round((240-rank)*1.25/240, 5) as Weights \\\n",
    "                from fast_scoring_ability t1')\n",
    "fsa.registerTempTable('fast_scoring_ability')\n",
    "fsa.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistency = Total Runs/Number of Times Out\n",
    "con = spark.sql('select t1.batsman as Batsman, round(t1.Total_runs/t2.no_of_times_dismissed, 5) as Consistency \\\n",
    "                from (select batsman,sum(batsman_runs) as Total_runs \\\n",
    "                from analysis_db group by batsman) t1 \\\n",
    "                inner join \\\n",
    "                (select batsman, count(*) as no_of_times_dismissed \\\n",
    "                from analysis_db where player_dismissed is not null \\\n",
    "                group by batsman) t2 on t1.batsman=t2.batsman')\n",
    "con.registerTempTable('consistency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = spark.sql('select rank() over (order by consistency desc) as Rank, t1.* \\\n",
    "                  from consistency t1 \\\n",
    "                  inner join \\\n",
    "                  no_of_matches_table t2 \\\n",
    "                  on t1.batsman = t2.batsman where no_of_matches>9')\n",
    "con.registerTempTable('consistency')\n",
    "con.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = spark.sql('select t1.*, round((240-rank)/240, 5) as Points, round((240-rank)/240, 5) as Weights \\\n",
    "                from consistency t1')\n",
    "con.registerTempTable('consistency')\n",
    "con.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Running Between Wickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Between Wickets = (Total Runs – (4*Fours + 6*Sixes))/(Total Balls Played – Boundary Balls)\n",
    "rbw = spark.sql('select t9.batsman as Batsman, round(nvl(t8.running_between_wickets,0), 5) as Running_Between_Wickets \\\n",
    "                from (select t4.batsman, t4.first_bracket/t7.second_bracket as Running_Between_Wickets \\\n",
    "                from (select t1.batsman, t3.total_runs-(t1.fours*4 + t2.sixes*6) as first_bracket \\\n",
    "                from (select batsman,count(*) as fours from analysis_db where batsman_runs = 4 \\\n",
    "                group by batsman) t1 \\\n",
    "                inner join \\\n",
    "                (select batsman,count(*) as sixes from analysis_db where batsman_runs = 6 group by batsman) t2 \\\n",
    "                on t1.batsman=t2.batsman \\\n",
    "                inner join \\\n",
    "                (select batsman,sum(batsman_runs) as total_runs from analysis_db group by batsman) t3 \\\n",
    "                on t3.batsman=t1.batsman) t4 \\\n",
    "                inner join\\\n",
    "                (select t5.batsman, t5.total_balls_played-t6.boundry_balls as second_bracket from \\\n",
    "                (select batsman, count(*) as total_balls_played from analysis_db group by batsman) t5 \\\n",
    "                inner join \\\n",
    "                (select batsman, count(*) as boundry_balls from analysis_db where batsman_runs=4 or batsman_runs=6 group by batsman) t6\\\n",
    "                on t5.batsman=t6.batsman) t7 \\\n",
    "                on t4.batsman=t7.batsman) t8 \\\n",
    "                right join \\\n",
    "                no_of_matches_table t9 \\\n",
    "                on t8.batsman = t9.batsman')\n",
    "rbw.registerTempTable('running_between_wickets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbw = spark.sql('select rank() over (order by running_between_wickets desc) as Rank, t1.* \\\n",
    "                  from running_between_wickets t1 \\\n",
    "                  inner join \\\n",
    "                  no_of_matches_table t2\\\n",
    "                  on t1.batsman = t2.batsman where no_of_matches>9')\n",
    "rbw.registerTempTable('running_between_wickets')\n",
    "rbw.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbw = spark.sql('select t1.*, round((240-rank)/240, 5) as Points, round((240-rank)/240, 5) as Weights \\\n",
    "                from running_between_wickets t1')\n",
    "rbw.registerTempTable('running_between_wickets')\n",
    "rbw.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table Name for each Metric\n",
    "# Hard Hitting Ability: hard_hitting_ability\n",
    "# Finisher: finisher\n",
    "# Fast Scoring Ability: fast_scoring_ability\n",
    "# Consistency: consistency\n",
    "# Running Between Wickets: running_between_wickets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Total Batting Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batting_weight = spark.sql('select hht.Batsman, round((hht.Weights+f.Weights+fsa.Weights+c.Weights+rbw.Weights), 5) as Total_Batting_Weights \\\n",
    "                                 from hard_hitting_ability hht \\\n",
    "                                 inner join finisher f \\\n",
    "                                 on hht.Batsman = f.Batsman \\\n",
    "                                 inner join fast_scoring_ability fsa \\\n",
    "                                 on hht.Batsman = fsa.Batsman \\\n",
    "                                 inner join consistency c \\\n",
    "                                 on hht.Batsman = c.Batsman \\\n",
    "                                 inner join running_between_wickets rbw \\\n",
    "                                 on hht.Batsman = rbw.Batsman \\\n",
    "                                 order by Total_Batting_Weights desc')\n",
    "total_batting_weight.registerTempTable('total_batting_weight')\n",
    "total_batting_weight.show(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping intermediate tables\n",
    "table_names = ['no_of_matches_table', 'hard_hitting_ability', 'finisher', 'fast_scoring_ability', 'consistency', 'running_between_wickets']\n",
    "for table in table_names:\n",
    "    cmd = 'drop table if exists {}'.format(table)\n",
    "    drop = spark.sql(cmd)\n",
    "check = spark.sql('show tables')\n",
    "check.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bowling Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nmbo: no. of bowlers\n",
    "# nmb: no. of matches played by a bowler\n",
    "# eco: economy\n",
    "# wta: wicket taking ability\n",
    "# cons: consistency\n",
    "# cwta: crucial wicket taking ability\n",
    "# spi: short performance index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Bowlers\n",
    "nmbo = spark.sql('Select count(distinct(bowler)) as No_of_Bowlers from analysis_db')\n",
    "nmbo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of matches played by a bowler\n",
    "nmb = spark.sql('select bowler as Bowler, count(distinct(match_id)) as No_of_Matches from analysis_db group by bowler')\n",
    "nmb.registerTempTable('no_of_matches_bowlers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Economy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Economy = Runs Scored/(Number of balls bowled by bowler/6)\n",
    "eco = spark.sql('Select bowler as Bowler, round(runs/overs, 5) as Economy \\\n",
    "                from (Select bowler,round(count(*)/6) \\\n",
    "                as overs,sum(total_runs) as runs \\\n",
    "                from analysis_db \\\n",
    "                group by bowler)')\n",
    "eco.registerTempTable('economy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eco = spark.sql('select row_number() over (order by e.Economy asc) as Rank, e.*,n.No_of_Matches \\\n",
    "                from economy e \\\n",
    "                inner join \\\n",
    "                no_of_matches_bowlers n \\\n",
    "                on e.Bowler = n.Bowler where n.No_of_Matches>9')\n",
    "eco.registerTempTable('economy')\n",
    "eco.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eco = spark.sql('select *, round((212 - Rank)/212, 5) as Points, round((212 - Rank)*1.5/212, 5) as Weight from economy')\n",
    "eco.registerTempTable('economy')\n",
    "eco.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wicket Taking Ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wicket Taking Ability = Number of balls bowled/Wickets Taken\n",
    "wta = spark.sql('(Select t1.bowler as Bowler, round(t2.balls/t1.wickets, 5) as Wicket_Taking_Ability from \\\n",
    "                (Select bowler,count(*) as wickets from analysis_db where player_dismissed is not null \\\n",
    "                and (dismissal_kind = \\'bowled\\' or  dismissal_kind = \\'hit wicket\\' \\\n",
    "                or  dismissal_kind = \\'stumped\\' or  dismissal_kind = \\'lbw\\' \\\n",
    "                or  dismissal_kind = \\'caught and bowled\\' or  dismissal_kind = \\'caught\\') \\\n",
    "                group by bowler) t1 \\\n",
    "                inner join \\\n",
    "                (select count(*) as balls,bowler from analysis_db group by bowler)t2 on \\\n",
    "                t1.bowler = t2.bowler)')\n",
    "wta.registerTempTable('wicket_taking_ability')\n",
    "wta.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wta = spark.sql('select row_number() over (order by w.Wicket_Taking_Ability asc) as Rank, w.*, n.No_of_Matches \\\n",
    "                from wicket_taking_ability w \\\n",
    "                inner join \\\n",
    "                no_of_matches_bowlers n on \\\n",
    "                w.Bowler = n.Bowler where n.No_of_Matches > 9')\n",
    "wta.registerTempTable('wicket_taking_ability')\n",
    "wta.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wta = spark.sql('select *,round((212-Rank)/212, 5) as Points,round(1.5*(212-Rank)/212, 5) as Weight \\\n",
    "                from wicket_taking_ability')\n",
    "wta.registerTempTable('wicket_taking_ability')\n",
    "wta.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistency = Runs Conceded/Wickets Taken\n",
    "cons = spark.sql('select t1.bowler as Bowler, round(t1.runs/t2.wickets, 5) as Consistency \\\n",
    "                 from (select sum(total_runs) as runs,bowler from analysis_db group by bowler) t1 \\\n",
    "                 inner join \\\n",
    "                 (Select bowler,count(*) as wickets from analysis_db where player_dismissed is not null \\\n",
    "                 and (dismissal_kind = \\'bowled\\' or  dismissal_kind = \\'hit wicket\\' \\\n",
    "                 or dismissal_kind = \\'stumped\\' or  dismissal_kind = \\'lbw\\' \\\n",
    "                 or dismissal_kind = \\'caught and bowled\\' or  dismissal_kind = \\'caught\\') \\\n",
    "                 group by bowler)t2 on t1.bowler = t2.bowler')\n",
    "cons.registerTempTable('consistency')\n",
    "cons.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = spark.sql('select row_number() over (order by c.Consistency asc) as Rank, c.*,n.No_of_Matches \\\n",
    "                 from consistency c \\\n",
    "                 inner join \\\n",
    "                 no_of_matches_bowlers n on \\\n",
    "                 c.Bowler = n.Bowler where n.No_of_Matches > 9')\n",
    "cons.registerTempTable('consistency')\n",
    "cons.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = spark.sql('select *,round((212-Rank)/212, 5) as Points, round((212-Rank)/212, 5) as Weights from consistency')\n",
    "cons.registerTempTable('consistency')\n",
    "cons.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crucial Wicket Taking Ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crucial Wicket Taking Ability = Number of times Four or Five Wickets Taken/Number of Innings Played\n",
    "cwta = spark.sql('select t2.bowler as Bowler, round(nvl(t1.no_of_4wickets/t2.innings,0), 5) as Crucial_Wicket_Taking_Ablity \\\n",
    "                 from (select bowler,count(*) as no_of_4wickets from (select * from \\\n",
    "                 (select match_id,bowler,count(*) as wickets from analysis_db where player_dismissed \\\n",
    "                 is not null \\\n",
    "                 and (dismissal_kind = \\'bowled\\' or  dismissal_kind = \\'hit wicket\\' \\\n",
    "                 or  dismissal_kind = \\'stumped\\' or  dismissal_kind = \\'lbw\\' \\\n",
    "                 or  dismissal_kind = \\'caught and bowled\\' or  dismissal_kind = \\'caught\\') \\\n",
    "                 group by bowler,match_id ) \\\n",
    "                 where wickets > 3) group by bowler)t1 \\\n",
    "                 right join \\\n",
    "                 (select bowler,count(match_id) as \\\n",
    "                 innings from (select distinct(match_id),bowler from analysis_db) \\\n",
    "                 group by bowler)t2 \\\n",
    "                 on t1.bowler = t2.bowler order by Crucial_Wicket_Taking_Ablity desc')\n",
    "cwta.registerTempTable('crucial_wicket_taking_ablity')\n",
    "cwta.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwta = spark.sql('select rank() over (order by cw.Crucial_Wicket_Taking_Ablity desc) as Rank, cw.*,n.No_of_Matches \\\n",
    "                 from crucial_wicket_taking_ablity cw \\\n",
    "                 inner join no_of_matches_bowlers n on \\\n",
    "                 cw.Bowler = n.Bowler where n.No_of_Matches > 9')\n",
    "cwta.registerTempTable('crucial_wicket_taking_ablity')\n",
    "cwta.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwta = spark.sql('select *,round((212-Rank)/212, 5) as Points, round(1.5*(212-Rank)/212, 5) as Weights \\\n",
    "                 from crucial_wicket_taking_ablity')\n",
    "cwta.registerTempTable('crucial_wicket_taking_ablity')\n",
    "cwta.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Performance Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short Performance Index = (Wickets Taken – 4* Number of Times Four Wickets Taken – 5* Number of Times Five Wickets Taken)/(Innings Played – Number of Times Four Wickets or Five Wickets Taken)\n",
    "spi = spark.sql('select n.bowler as Bowler, round(nvl(t5.Short_Performance_Index,0), 5) as Short_Performance_Index \\\n",
    "                from (select t1.bowler,(t3.wickets - 4*t1.no_of_4wickets - 5*t2.no_of_4wickets)/ \\\n",
    "                (t4.innings - t1.no_of_4wickets - t2.no_of_4wickets) as Short_Performance_Index \\\n",
    "                from (select bowler,count(*) as no_of_4wickets \\\n",
    "                from (select * from (select match_id,bowler,count(*) as wickets from analysis_db where player_dismissed \\\n",
    "                is not null \\\n",
    "                and (dismissal_kind = \\'bowled\\' or  dismissal_kind = \\'hit wicket\\' \\\n",
    "                or  dismissal_kind = \\'stumped\\' or  dismissal_kind = \\'lbw\\' \\\n",
    "                or  dismissal_kind = \\'caught and bowled\\' or  dismissal_kind = \\'caught\\')\\\n",
    "                group by bowler, match_id ) \\\n",
    "                where wickets = 4) group by bowler) t1 \\\n",
    "                inner join \\\n",
    "                (select bowler,count(*) as no_of_4wickets from (select * from \\\n",
    "                (select match_id,bowler,count(*) as wickets from analysis_db where player_dismissed \\\n",
    "                is not null \\\n",
    "                and (dismissal_kind = \\'bowled\\' or  dismissal_kind = \\'hit wicket\\' \\\n",
    "                or dismissal_kind = \\'stumped\\' or  dismissal_kind = \\'lbw\\' \\\n",
    "                or dismissal_kind = \\'caught and bowled\\' or  dismissal_kind = \\'caught\\')\\\n",
    "                group by bowler,match_id ) \\\n",
    "                where wickets = 5) group by bowler) t2 \\\n",
    "                inner join \\\n",
    "                (select bowler,count(*) as wickets from analysis_db where player_dismissed is not null \\\n",
    "                and (dismissal_kind = \\'bowled\\' or  dismissal_kind = \\'hit wicket\\' \\\n",
    "                or  dismissal_kind = \\'stumped\\' or  dismissal_kind = \\'lbw\\' \\\n",
    "                or  dismissal_kind = \\'caught and bowled\\' or  dismissal_kind = \\'caught\\') \\\n",
    "                group by bowler) t3 \\\n",
    "                inner join \\\n",
    "                (select bowler,count(match_id) as \\\n",
    "                innings from (select distinct(match_id),bowler from analysis_db) group by bowler) t4 \\\n",
    "                on t1.bowler = t2.bowler and t1.bowler = t3.bowler and t1.bowler = t4.bowler) t5 \\\n",
    "                right join \\\n",
    "                no_of_matches_bowlers n on t5.Bowler = n.Bowler order by Short_Performance_Index desc')\n",
    "spi.registerTempTable('short_performance_index')\n",
    "spi.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spi = spark.sql('select rank() over (order by sp.Short_Performance_Index desc) as Rank, sp.*, n.No_of_Matches \\\n",
    "                from short_performance_index sp \\\n",
    "                inner join \\\n",
    "                no_of_matches_bowlers n on \\\n",
    "                sp.Bowler = n.Bowler where n.No_of_Matches > 9')\n",
    "spi.registerTempTable('short_performance_index')\n",
    "spi.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spi = spark.sql('select *,round((212-Rank)/212, 5) as Points, round((212-Rank)/212, 5) as Weights \\\n",
    "                from short_performance_index')\n",
    "spi.registerTempTable('short_performance_index')\n",
    "spi.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table Name for each Metric\n",
    "# Economy: economy\n",
    "# Wicket Taking Ability: wicket_taking_ability\n",
    "# Consistency: consistency\n",
    "# Crucial Wicket Taking Ablity: crucial_wicket_taking_ablity\n",
    "# Short Performance Index: short_performance_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Bowling Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bowling_weight = spark.sql('select e.bowler as Bowler, round((e.Weight+wta.Weight+c.Weights+cwta.Weights+spi.Weights), 5) as Total_Bowling_Weights \\\n",
    "                                 from economy e \\\n",
    "                                 inner join wicket_taking_ability wta \\\n",
    "                                 on e.Bowler = wta.Bowler \\\n",
    "                                 inner join consistency c \\\n",
    "                                 on e.Bowler = c.Bowler \\\n",
    "                                 inner join crucial_wicket_taking_ablity cwta \\\n",
    "                                 on e.Bowler = cwta.Bowler \\\n",
    "                                 inner join short_performance_index spi \\\n",
    "                                 on e.Bowler = spi.Bowler \\\n",
    "                                 order by Total_Bowling_Weights desc')\n",
    "total_bowling_weight.registerTempTable('total_bowling_weight')\n",
    "total_bowling_weight.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping intermediate Tables\n",
    "table_names = ['no_of_matches_bowlers', 'economy', 'wicket_taking_ability', 'consistency', 'crucial_wicket_taking_ablity', 'short_performance_index']\n",
    "for table in table_names:\n",
    "    cmd = 'drop table if exists {}'.format(table)\n",
    "    drop = spark.sql(cmd)\n",
    "check = spark.sql('show tables')\n",
    "check.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Weights per Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weight = spark.sql('select *, (coalesce(total_batting_weight, 0) + coalesce(total_bowling_weight, 0)) as Total_Weight\\\n",
    "                          from(select t1.batsman as Player, nvl(t1.Total_Batting_Weights,0) as Total_Batting_Weight, \\\n",
    "                          nvl(t2.Total_Bowling_Weights,0) as Total_Bowling_Weight\\\n",
    "                          from total_batting_weight t1 \\\n",
    "                          full outer join total_bowling_weight t2 \\\n",
    "                          on t1.batsman = t2.bowler) \\\n",
    "                          order by Total_Weight desc')\n",
    "total_weight.registerTempTable('total_weight')\n",
    "total_weight.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping intermediate tables\n",
    "table_names = ['total_batting_weight', 'total_bowling_weight']\n",
    "for table in table_names:\n",
    "    cmd = 'drop table if exists {}'.format(table)\n",
    "    drop = spark.sql(cmd)\n",
    "check = spark.sql('show tables')\n",
    "check.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the player weight data\n",
    "total_weight.toPandas().to_csv(path + '/dataset/weights_data/total_weights.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.ufunc size changed, may indicate binary incompatibility. Expected 124 from C header, got 112 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-5e66aa326498>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Importing Libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m _DEFAULT_TAGS = {\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m__init__.pxd\u001b[0m in \u001b[0;36minit sklearn.utils.murmurhash\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.ufunc size changed, may indicate binary incompatibility. Expected 124 from C header, got 112 from PyObject"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "matches = pd.read_csv(path + '/dataset/clean_data/matches.csv')\n",
    "deliveries = pd.read_csv(path + '/dataset/clean_data/deliveries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = {'team1': {'Mumbai Indians':1,'Kolkata Knight Riders':2,'Royal Challengers Bangalore':3,\n",
    "                             'Deccan Chargers':4,'Chennai Super Kings':5,'Rajasthan Royals':6,'Delhi Daredevils':7,\n",
    "                             'Gujarat Lions':8,'Kings XI Punjab':9,'Sunrisers Hyderabad':10,'Rising Pune Supergiant':11,\n",
    "                             'Kochi Tuskers Kerala':12,'Pune Warriors':13, 'Delhi Capitals':14},\n",
    "                   'team2': {'Mumbai Indians':1,'Kolkata Knight Riders':2,'Royal Challengers Bangalore':3,\n",
    "                             'Deccan Chargers':4,'Chennai Super Kings':5,'Rajasthan Royals':6,'Delhi Daredevils':7,\n",
    "                             'Gujarat Lions':8,'Kings XI Punjab':9,'Sunrisers Hyderabad':10,'Rising Pune Supergiant':11,\n",
    "                             'Kochi Tuskers Kerala':12,'Pune Warriors':13, 'Delhi Capitals':14},\n",
    "                   'toss_winner': {'Mumbai Indians':1,'Kolkata Knight Riders':2,'Royal Challengers Bangalore':3,\n",
    "                                   'Deccan Chargers':4,'Chennai Super Kings':5,'Rajasthan Royals':6,'Delhi Daredevils':7,\n",
    "                                   'Gujarat Lions':8,'Kings XI Punjab':9,'Sunrisers Hyderabad':10,'Rising Pune Supergiant':11,\n",
    "                                   'Kochi Tuskers Kerala':12,'Pune Warriors':13, 'Delhi Capitals':14},\n",
    "                   'winner': {'Mumbai Indians':1,'Kolkata Knight Riders':2,'Royal Challengers Bangalore':3,'Deccan Chargers':4,\n",
    "                              'Chennai Super Kings':5,'Rajasthan Royals':6,'Delhi Daredevils':7,'Gujarat Lions':8,\n",
    "                              'Kings XI Punjab':9,'Sunrisers Hyderabad':10,'Rising Pune Supergiant':11,'Kochi Tuskers Kerala':12,\n",
    "                              'Pune Warriors':13, 'Delhi Capitals':14, 'Draw':15}}\n",
    "matches.replace(encode, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the encoding result\n",
    "matches.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matches[['team1','team2','city','toss_decision','toss_winner','venue','winner','season']]\n",
    "df = pd.DataFrame(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
