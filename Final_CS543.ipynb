{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName('Ops').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"E:/Rutgers/Projects/MDSR/IPL-MSDR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Data\n",
    "matches = pd.read_csv(path + '/dataset/original_ipldata/matches.csv')\n",
    "deliveries = pd.read_csv(path + '/dataset/original_ipldata/deliveries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema of original data (matches.csv)\n",
    "matches.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema of original data (deliveries.csv)\n",
    "deliveries.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns that are of no use\n",
    "matches = matches.drop(columns = ['umpire1', 'umpire2','umpire3','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filing empty values\n",
    "matches = matches.fillna(value = 'None')\n",
    "deliveries = deliveries.fillna(value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema of cleaned data (matches.csv)\n",
    "matches.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema of cleaned data (deliveries.csv)\n",
    "deliveries.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving cleaned data (matches.csv)\n",
    "matches.to_csv(path + '/dataset/clean_data/matches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving cleaned data (deliveries.csv)\n",
    "deliveries.to_csv(path + '/dataset/clean_data/deliveries.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teams playing in the league\n",
    "teams = matches['team1'].unique()\n",
    "print(\"Total number of teams participated so far: \" + str(len(matches['team1'].unique())))\n",
    "print(\"Teams participated so far: \")\n",
    "for i in teams:\n",
    "    print(\"- \" + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Venues\n",
    "print(\"Number of venues matches were played: \" + str(len(matches['venue'].unique())))\n",
    "for i in matches['venue'].unique():\n",
    "    print(\"- \" + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cities the matches were played\n",
    "print(\"Number of cities matches were played: \" + str(len(matches['city'].unique())))\n",
    "for i in matches['city'].unique():\n",
    "    print(\"- \" + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of bowlers so far\n",
    "print(\"Total number of bowlers: \" + str(len(deliveries['bowler'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of batsmen so far\n",
    "print(\"Total number of batsmen: \" + str(len(deliveries['batsman'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of participating players\n",
    "players = set()\n",
    "for i in range(len(deliveries['match_id'])):\n",
    "    players.add(deliveries['bowler'][i])\n",
    "    players.add(deliveries['batsman'][i])\n",
    "    players.add(deliveries['non_striker'][i])\n",
    "print(\"Total number of player: \" + str(len(players)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = spark.read.csv(path + '/dataset/clean_data/matches.csv',inferSchema=True,header=True)\n",
    "deliveries = spark.read.csv(path + '/dataset/clean_data/deliveries.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total number of matches per season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.registerTempTable('seasons')\n",
    "seasons = spark.sql('''Select distinct(season),count(*) as total_matches from seasons group by season ''') \n",
    "seasons.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, a = plt.subplots()\n",
    "a = sns.barplot(x =\"season\", y=\"total_matches\", data=seasons.toPandas(),palette='viridis')\n",
    "a.set_xlabel('Season')\n",
    "a.set_ylabel('Total Matches')\n",
    "a.set_title('Number of matches in each season')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of maches played by each team since season 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.registerTempTable('team')\n",
    "team = spark.sql('''Select distinct(team), count(*) as total_matches from (Select team1 as team from team UNION ALL (select team2 as team from team)) group by team ''')\n",
    "team.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, a = plt.subplots(figsize = (5,5))\n",
    "a = sns.barplot(x =\"total_matches\", y=\"team\", data=team.toPandas(), palette='viridis')\n",
    "a.set_ylabel('Team')\n",
    "a.set_xlabel('Total Matches')\n",
    "a.set_title('Number of matches played by each team')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total season in which teams have played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.registerTempTable('team_season')\n",
    "team_season = spark.sql('''Select team1 as team, min(season) as first_season, max(season) as last_season, count(distinct(season)) as total_seasons from team_season group by team1 order by total_seasons desc''')\n",
    "team_season.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total number of matches won by teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.registerTempTable('most_win')\n",
    "most_win = spark.sql('''Select distinct(winner) as team, count(*) as total_matches from most_win where winner <>'None' group by winner order by total_matches ''')\n",
    "most_win.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, a = plt.subplots(figsize = (5,5))\n",
    "a = sns.barplot(x =\"total_matches\", y=\"team\", data=most_win.toPandas(), palette='viridis')\n",
    "a.set_ylabel('Team')\n",
    "a.set_xlabel('Total Matches')\n",
    "a.set_title('Number of matches won by each team')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total matches won by teams in each season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.registerTempTable('most_win_by_season')\n",
    "most_win_by_season = spark.sql('''Select season, winner as team, count(*) as total_matches_won from most_win_by_season where winner <> 'None' group by season, winner order by total_matches_won desc''')\n",
    "most_win_by_season.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Players with maximum man of the match awards "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.registerTempTable('man_match')\n",
    "man_match = spark.sql('''Select distinct(player_of_match), count(*) as total_matches from man_match group by player_of_match order by total_matches desc limit 10 ''')\n",
    "man_match.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, a = plt.subplots(figsize = (5,5))\n",
    "a = sns.barplot(x =\"total_matches\", y=\"player_of_match\", data=man_match.toPandas(), palette='viridis')\n",
    "a.set_xlabel('Total Matches')\n",
    "a.set_ylabel('Player')\n",
    "a.set_title('Number of times player won man of the match')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of matches per Venue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.registerTempTable('venue')\n",
    "venue = spark.sql('''Select distinct(venue), count(*) as total_matches from venue group by venue''')\n",
    "venue.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, a = plt.subplots(figsize = (10,20))\n",
    "a = sns.barplot(x =\"total_matches\", y=\"venue\", data=venue.toPandas(), palette='viridis')\n",
    "a.set_ylabel('Venue')\n",
    "a.set_xlabel('Total Matches')\n",
    "a.set_title('Number of matches at each venue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage toss decisions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.registerTempTable('toss')\n",
    "toss = spark.sql('''Select distinct(toss_decision), ((count(toss_decision)*100)/ (select count(*) from toss)) as percentage_count from toss group by toss_decision''')\n",
    "toss.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, a = plt.subplots(figsize = (5,5))\n",
    "a = sns.barplot(x =\"toss_decision\", y=\"percentage_count\", data=toss.toPandas(), palette='viridis')\n",
    "a.set_ylabel('Percentage')\n",
    "a.set_xlabel('Toss Decision')\n",
    "a.set_title('Percentage Plot of toss_decision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage of team winning the toss as well as the match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches.registerTempTable('toss_and_won')\n",
    "matches.registerTempTable('toss_won_data')\n",
    "toss_won_data = spark.sql('''Select t1.season, t1.total_matches, \\\n",
    "          t2.count_toss_and_won as count_toss_and_won, \\\n",
    "          (t2.count_toss_and_won / t1.total_matches * 100) as percent_toss_and_won from \\\n",
    "          (Select distinct(season),count(*) as total_matches from seasons group by season)t1 \\\n",
    "          left join (Select distinct(season), count(*) as count_toss_and_won from toss_and_won where toss_winner = winner group by season)t2 on t1.season = t2.season order by season''')\n",
    "toss_won_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, a = plt.subplots(figsize = (10,5))\n",
    "a = sns.barplot(x =\"season\", y=\"percent_toss_and_won\", data=toss_won_data.toPandas(), palette='viridis')\n",
    "a.set_ylabel('Percentage')\n",
    "a.set_xlabel('Season')\n",
    "a.set_title('Percentage Plot of Season and Toss_and_won')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage matches won by batting first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_batting_first = spark.sql('''Select t1.season, t1.total_matches, \\\n",
    "          t2.win_batting_first as win_batting_first, \\\n",
    "          (t2.win_batting_first/ t1.total_matches * 100) as percent_win_batting_first from \\\n",
    "          (Select distinct(season),count(*) as total_matches from seasons group by season)t1 \\\n",
    "          left join (Select distinct(season), count(*) as win_batting_first from seasons where win_by_runs > 0  group by season)t2 on t1.season = t2.season order by season ''')\n",
    "win_batting_first.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, a = plt.subplots(figsize = (10,5))\n",
    "a = sns.barplot(x =\"season\", y=\"percent_win_batting_first\", data=win_batting_first.toPandas(), palette='viridis')\n",
    "a.set_ylabel('Percentage')\n",
    "a.set_xlabel('Season')\n",
    "a.set_title('Percentage Plot of Season and won by batting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage matches won by fielding first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_bowling_first = spark.sql('''Select t1.season, t1.total_matches, \\\n",
    "          t2.win_bowling_first as win_bowling_first, \\\n",
    "          (t2.win_bowling_first/ t1.total_matches * 100) as percent_win_bowling_first from \\\n",
    "          (Select distinct(season),count(*) as total_matches from seasons group by season)t1 \\\n",
    "          left join (Select distinct(season), count(*) as win_bowling_first from seasons where win_by_wickets > 0  group by season)t2 on t1.season = t2.season order by season ''')\n",
    "win_bowling_first.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig, a = plt.subplots(figsize = (10,5))\n",
    "a = sns.barplot(x =\"season\", y=\"percent_win_bowling_first\", data=win_bowling_first.toPandas(), palette='viridis')\n",
    "a.set_ylabel('Percentage')\n",
    "a.set_xlabel('Season')\n",
    "a.set_title('Percentage Plot of Season and won by wickets ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "matches = spark.read.csv(path + '/dataset/clean_data/matches.csv',inferSchema=True,header=True)\n",
    "deliveries = spark.read.csv(path + '/dataset/clean_data/deliveries.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating temporary tables of the data\n",
    "matches.registerTempTable('matches_db')\n",
    "deliveries.registerTempTable('deliveries_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging both the tables\n",
    "merged_db = spark.sql('select m.*,d.* from matches_db as m inner join deliveries_db as d on m.id=d.match_id')\n",
    "merged_db.registerTempTable('analysis_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batting Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nmba: no. of batsmen\n",
    "# nm: no. of matches played by a batsman\n",
    "# hha: hard hitting ability\n",
    "# f: finisher\n",
    "# fsa: fast scoring ability\n",
    "# con: consistency\n",
    "# rbw: running between wickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Batsmen\n",
    "nmba = spark.sql('select count(distinct(batsman)) as No_of_Batsman from analysis_db')\n",
    "nmba.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Matches played by a batsmen\n",
    "nm = spark.sql('select batsman, count(distinct(match_id)) as No_of_Matches \\\n",
    "                from analysis_db group by batsman')\n",
    "nm.registerTempTable('no_of_matches_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Hitting Ability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard Hitting Ability = (4*Fours + 6*Sixes)/Balls Played by Batsman\n",
    "hha = spark.sql('select nmt.batsman as Batsman, nvl(t4.hard_hitting_ability,0) as \\\n",
    "                Hard_Hitting_Ability from \\\n",
    "                (select t1.batsman, (t1.fours*4 + t2.sixes*6)/t3.balls_played as hard_hitting_ability\\\n",
    "                from (select batsman,count(*) as fours from analysis_db where batsman_runs = 4 group by batsman) t1 \\\n",
    "                inner join  \\\n",
    "                (select batsman,count(*) as sixes from analysis_db where batsman_runs = 6 \\\n",
    "                group by batsman) t2 on t1.batsman=t2.batsman\\\n",
    "                inner join\\\n",
    "                (select batsman,count(*) as balls_played from analysis_db \\\n",
    "                group by batsman) t3 on t3.batsman=t1.batsman) t4 \\\n",
    "                right join no_of_matches_table nmt on t4.batsman = nmt.batsman')\n",
    "hha.registerTempTable('hard_hitting_ability_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hha = spark.sql('select rank() over (order by Hard_Hitting_Ability desc) as Rank, t1.* \\\n",
    "                  from hard_hitting_ability_table t1 \\\n",
    "                  inner join \\\n",
    "                  no_of_matches_table t2\\\n",
    "                  on t1.batsman = t2.batsman where no_of_matches>9')\n",
    "hha.registerTempTable('hard_hitting_ability_rank')\n",
    "hha.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hha = spark.sql('select t1.*, (240-rank)/240 as Points from hard_hitting_ability_rank t1')\n",
    "hha.registerTempTable('hard_hitting_ability_points')\n",
    "hha.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hha = spark.sql('select t1.*, Points*1.25 as Weight from hard_hitting_ability_points t1')\n",
    "hha.registerTempTable('hard_hitting_ability_weights')\n",
    "hha.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finisher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finisher = Not Out innings/Total Innings played\n",
    "f = spark.sql('select t3.batsman as Batsman, t3.not_out_innings/t4.total_matches_played as Finisher from\\\n",
    "              (select t1.batsman, t1.matches_played-t2.number_of_times_out as not_out_innings from \\\n",
    "              (select batsman, count(distinct(match_id)) as matches_played from analysis_db group by batsman) t1\\\n",
    "              inner join \\\n",
    "              (select batsman, count(*) as number_of_times_out from analysis_db where player_dismissed = batsman group by batsman) t2\\\n",
    "              on t1.batsman=t2.batsman) t3\\\n",
    "              inner join\\\n",
    "              (select batsman, count(distinct(match_id)) as total_matches_played \\\n",
    "              from analysis_db group by batsman) t4\\\n",
    "              on t3.batsman = t4.batsman')\n",
    "f.registerTempTable('finisher_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = spark.sql('select rank() over (order by finisher desc) as Rank, t1.* \\\n",
    "              from finisher_table t1 \\\n",
    "              inner join \\\n",
    "              no_of_matches_table t2 \\\n",
    "              on t1.batsman = t2.batsman \\\n",
    "              where no_of_matches>9')\n",
    "f.registerTempTable('finisher_rank')\n",
    "f.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = spark.sql('select t1.*, (240-rank)/240 as Points from finisher_rank t1')\n",
    "f.registerTempTable('finisher_points')\n",
    "f.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = spark.sql('select *, Points*1.25 as Weight from finisher_points')\n",
    "f.registerTempTable('finisher_weights')\n",
    "f.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Scoring Ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast Scoring Ability = Total Runs/Balls Played by Batsman\n",
    "fsa = spark.sql('select batsman as Batsman, Total_Runs/balls_played as Fast_Scoring_Ability \\\n",
    "                  from (select batsman,sum(batsman_runs) as Total_Runs, count(*) as balls_played \\\n",
    "                  from analysis_db group by batsman)')\n",
    "fsa.registerTempTable('fast_scoring_ability_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsa = spark.sql('select rank() over (order by fast_scoring_ability desc) as Rank, t1.* \\\n",
    "                  from fast_scoring_ability_table t1 \\\n",
    "                  inner join \\\n",
    "                  no_of_matches_table t2 \\\n",
    "                  on t1.batsman = t2.batsman where no_of_matches>9')\n",
    "fsa.registerTempTable('fast_scoring_ability_rank')\n",
    "fsa.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsa = spark.sql('select t1.*, (240-rank)/240 as Points \\\n",
    "                from fast_scoring_ability_rank t1')\n",
    "fsa.registerTempTable('fast_scoring_ability_points')\n",
    "fsa.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsa = spark.sql('select t1.*, Points*1.25 as Weight from fast_scoring_ability_points t1')\n",
    "fsa.registerTempTable('fast_scoring_ability_weights')\n",
    "fsa.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consistency = Total Runs/Number of Times Out\n",
    "con = spark.sql('select t1.batsman as Batsman,t1.Total_runs/t2.no_of_times_dismissed as Consistency \\\n",
    "                        from (select batsman,sum(batsman_runs) as Total_runs \\\n",
    "                        from analysis_db group by batsman) t1 \\\n",
    "                        inner join \\\n",
    "                        (select batsman, count(*) as no_of_times_dismissed \\\n",
    "                        from analysis_db where player_dismissed is not null \\\n",
    "                        group by batsman) t2 on t1.batsman=t2.batsman')\n",
    "con.registerTempTable('consistency_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = spark.sql('select rank() over (order by consistency desc) as Rank, t1.* \\\n",
    "                  from consistency_table t1 \\\n",
    "                  inner join \\\n",
    "                  no_of_matches_table t2 \\\n",
    "                  on t1.batsman = t2.batsman where no_of_matches>9')\n",
    "con.registerTempTable('consistency_rank')\n",
    "con.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = spark.sql('select t1.*, (240-rank)/240 as Points from consistency_rank t1')\n",
    "con.registerTempTable('consistency_points')\n",
    "con.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = spark.sql('select t1.*, Points as Weight from consistency_points t1')\n",
    "con.registerTempTable('consistency_weights')\n",
    "con.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Running Between Wickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Between Wickets = (Total Runs – (4*Fours + 6*Sixes))/(Total Balls Played – Boundary Balls)\n",
    "rbw = spark.sql('select t9.batsman as Batsman, nvl(t8.running_between_wickets,0) as Running_Between_Wickets from \\\n",
    "                (select t4.batsman, t4.first_bracket/t7.second_bracket as Running_Between_Wickets from \\\n",
    "                (select t1.batsman, t3.total_runs-(t1.fours*4 + t2.sixes*6) as first_bracket \\\n",
    "                from (select batsman,count(*) as fours from analysis_db where batsman_runs = 4 \\\n",
    "                group by batsman) t1 \\\n",
    "                inner join \\\n",
    "                (select batsman,count(*) as sixes from analysis_db where batsman_runs = 6 group by batsman) t2 \\\n",
    "                on t1.batsman=t2.batsman \\\n",
    "                inner join \\\n",
    "                (select batsman,sum(batsman_runs) as total_runs from analysis_db group by batsman) t3 \\\n",
    "                on t3.batsman=t1.batsman) t4 \\\n",
    "                inner join\\\n",
    "                (select t5.batsman, t5.total_balls_played-t6.boundry_balls as second_bracket from \\\n",
    "                (select batsman, count(*) as total_balls_played from analysis_db group by batsman) t5 \\\n",
    "                inner join \\\n",
    "                (select batsman, count(*) as boundry_balls from analysis_db where batsman_runs=4 or batsman_runs=6 group by batsman) t6\\\n",
    "                on t5.batsman=t6.batsman) t7 \\\n",
    "                on t4.batsman=t7.batsman) t8 \\\n",
    "                right join \\\n",
    "                no_of_matches_table t9 \\\n",
    "                on t8.batsman = t9.batsman')\n",
    "rbw.registerTempTable('running_between_wickets_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbw = spark.sql('select rank() over (order by running_between_wickets desc) as Rank, t1.* \\\n",
    "                  from running_between_wickets_table t1 \\\n",
    "                  inner join \\\n",
    "                  no_of_matches_table t2\\\n",
    "                  on t1.batsman = t2.batsman where no_of_matches>9')\n",
    "rbw.registerTempTable('running_between_wickets_rank')\n",
    "rbw.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbw = spark.sql('select t1.*, (240-rank)/240 as Points from running_between_wickets_rank t1')\n",
    "rbw.registerTempTable('running_between_wickets_points')\n",
    "rbw.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbw = spark.sql('select t1.*, Points as weight from running_between_wickets_points t1')\n",
    "rbw.registerTempTable('running_between_wickets_weights')\n",
    "rbw.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table Name for each Metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bowling Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nmbo: no. of bowlers\n",
    "# nmb: no. of matches played by a bowler\n",
    "# eco: economy\n",
    "# wta: wicket taking ability\n",
    "# cons: consistency\n",
    "# cwta: crucial wicket taking ability\n",
    "# spi: short performance index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|No_of_Bowlers|\n",
      "+-------------+\n",
      "|          405|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of Bowlers\n",
    "nmbo = spark.sql('Select count(distinct(bowler)) as No_of_Bowlers from analysis_db ')\n",
    "nmbo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of matches played by a bowler\n",
    "nmb = spark.sql('select bowler as Bowler, count(distinct(match_id)) as No_of_Matches from analysis_db group by bowler')\n",
    "nmb.registerTempTable('no_of_matches_bowlers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Economy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|        Bowler|Economy|\n",
      "+--------------+-------+\n",
      "| Kuldeep Yadav|8.37857|\n",
      "|    TM Dilshan|    8.0|\n",
      "|         A Roy|4.66667|\n",
      "|    KA Pollard|8.38028|\n",
      "|M Muralitharan|6.68561|\n",
      "| LA Carseldine|    6.0|\n",
      "|       J Botha| 6.9322|\n",
      "|      DR Smith|8.87097|\n",
      "|Jaskaran Singh|9.15789|\n",
      "|    A Flintoff|9.63636|\n",
      "+--------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Economy = Runs Scored/(Number of balls bowled by bowler/6)\n",
    "eco = spark.sql('Select bowler as Bowler, round(runs/overs, 5) as Economy \\\n",
    "                from (Select bowler,round(count(*)/6) \\\n",
    "                as overs,sum(total_runs) as runs \\\n",
    "                from analysis_db \\\n",
    "                group by bowler)')\n",
    "eco.registerTempTable('economy')\n",
    "eco.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+-------+-------------+\n",
      "|Rank|          Bowler|Economy|No_of_Matches|\n",
      "+----+----------------+-------+-------------+\n",
      "|   1|   Sohail Tanvir|   6.25|           11|\n",
      "|   2|      A Chandila|6.28205|           12|\n",
      "|   3|         J Yadav|6.52632|           12|\n",
      "|   4|      SM Pollock|6.53191|           13|\n",
      "|   5|        A Kumble|6.64024|           42|\n",
      "|   6|      GD McGrath|6.65455|           14|\n",
      "|   7|        DW Steyn|6.66848|           92|\n",
      "|   8|  M Muralitharan|6.68561|           66|\n",
      "|   9|RN ten Doeschate|6.71429|           10|\n",
      "|  10|       RD Chahar|6.71429|           15|\n",
      "+----+----------------+-------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eco = spark.sql('select row_number() over (order by e.Economy asc) as Rank, e.*,n.No_of_Matches \\\n",
    "                from economy e \\\n",
    "                inner join \\\n",
    "                no_of_matches_bowlers n \\\n",
    "                on e.Bowler = n.Bowler where n.No_of_Matches>9')\n",
    "eco.registerTempTable('economy')\n",
    "eco.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+-------+-------------+-------+--------+\n",
      "|Rank|          Bowler|Economy|No_of_Matches| Points|  Weight|\n",
      "+----+----------------+-------+-------------+-------+--------+\n",
      "|   1|   Sohail Tanvir|   6.25|           11|0.99528|1.492925|\n",
      "|   2|      A Chandila|6.28205|           12|0.99057|1.485849|\n",
      "|   3|         J Yadav|6.52632|           12|0.98585|1.478774|\n",
      "|   4|      SM Pollock|6.53191|           13|0.98113|1.471698|\n",
      "|   5|        A Kumble|6.64024|           42|0.97642|1.464623|\n",
      "|   6|      GD McGrath|6.65455|           14| 0.9717|1.457547|\n",
      "|   7|        DW Steyn|6.66848|           92|0.96698|1.450472|\n",
      "|   8|  M Muralitharan|6.68561|           66|0.96226|1.443396|\n",
      "|   9|RN ten Doeschate|6.71429|           10|0.95755|1.436321|\n",
      "|  10|       RD Chahar|6.71429|           15|0.95283|1.429245|\n",
      "+----+----------------+-------+-------------+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eco = spark.sql('select *, round((212 - Rank)/212, 5) as Points, (212 - Rank)*1.5/212 as Weight from economy')\n",
    "eco.registerTempTable('economy')\n",
    "eco.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wicket Taking Ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------+\n",
      "|        Bowler|Wicket_Taking_Ability|\n",
      "+--------------+---------------------+\n",
      "|         A Roy|                 15.0|\n",
      "| Kuldeep Yadav|             21.48718|\n",
      "|    TM Dilshan|                 55.0|\n",
      "|       J Botha|                28.36|\n",
      "|    KA Pollard|             22.78571|\n",
      "| LA Carseldine|                  7.0|\n",
      "|M Muralitharan|             24.70313|\n",
      "|      DR Smith|             21.42308|\n",
      "|Jaskaran Singh|                 18.5|\n",
      "|    A Flintoff|                 33.0|\n",
      "+--------------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Wicket Taking Ability = Number of balls bowled/Wickets Taken\n",
    "wta = spark.sql('(Select t1.bowler as Bowler, round(t2.balls/t1.wickets, 5) as Wicket_Taking_Ability from \\\n",
    "                (Select bowler,count(*) as wickets from analysis_db where player_dismissed is not null \\\n",
    "                and (dismissal_kind = \\'bowled\\' or  dismissal_kind = \\'hit wicket\\' \\\n",
    "                or  dismissal_kind = \\'stumped\\' or  dismissal_kind = \\'lbw\\' \\\n",
    "                or  dismissal_kind = \\'caught and bowled\\' or  dismissal_kind = \\'caught\\') \\\n",
    "                group by bowler) t1 \\\n",
    "                inner join \\\n",
    "                (select count(*) as balls,bowler from analysis_db group by bowler)t2 on \\\n",
    "                t1.bowler = t2.bowler)')\n",
    "wta.registerTempTable('wicket_taking_ability')\n",
    "wta.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+---------------------+-------------+\n",
      "|Rank|        Bowler|Wicket_Taking_Ability|No_of_Matches|\n",
      "+----+--------------+---------------------+-------------+\n",
      "|   1|       A Zampa|             11.84211|           11|\n",
      "|   2| Sohail Tanvir|             12.04545|           11|\n",
      "|   3|       K Ahmed|             12.68421|           10|\n",
      "|   4|        N Rana|             13.42857|           12|\n",
      "|   5|      K Rabada|                 14.0|           18|\n",
      "|   6|      BJ Hodge|                 14.0|           20|\n",
      "|   7|  CRD Fernando|             14.64706|           10|\n",
      "|   8|    YA Abdulla|                 14.8|           11|\n",
      "|   9|A Ashish Reddy|                 15.0|           20|\n",
      "|  10|       S Gopal|             15.60526|           30|\n",
      "+----+--------------+---------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wta = spark.sql('select row_number() over (order by w.Wicket_Taking_Ability asc) as Rank, w.*, n.No_of_Matches \\\n",
    "                from wicket_taking_ability w \\\n",
    "                inner join \\\n",
    "                no_of_matches_bowlers n on \\\n",
    "                w.Bowler = n.Bowler where n.No_of_Matches > 9')\n",
    "wta.registerTempTable('wicket_taking_ability')\n",
    "wta.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+---------------------+-------------+-------+-------+\n",
      "|Rank|         Bowler|Wicket_Taking_Ability|No_of_Matches| Points| Weight|\n",
      "+----+---------------+---------------------+-------------+-------+-------+\n",
      "|   1|        A Zampa|             11.84211|           11|0.99528|1.49293|\n",
      "|   2|  Sohail Tanvir|             12.04545|           11|0.99057|1.48585|\n",
      "|   3|        K Ahmed|             12.68421|           10|0.98585|1.47877|\n",
      "|   4|         N Rana|             13.42857|           12|0.98113|1.47170|\n",
      "|   5|       K Rabada|                 14.0|           18|0.97642|1.46462|\n",
      "|   6|       BJ Hodge|                 14.0|           20| 0.9717|1.45755|\n",
      "|   7|   CRD Fernando|             14.64706|           10|0.96698|1.45047|\n",
      "|   8|     YA Abdulla|                 14.8|           11|0.96226|1.44340|\n",
      "|   9| A Ashish Reddy|                 15.0|           20|0.95755|1.43632|\n",
      "|  10|        S Gopal|             15.60526|           30|0.95283|1.42925|\n",
      "|  11|    Imran Tahir|             15.81013|           55|0.94811|1.42217|\n",
      "|  12|      CR Woakes|                15.84|           18| 0.9434|1.41509|\n",
      "|  13|         AJ Tye|             15.84615|           26|0.93868|1.40802|\n",
      "|  14|     WPUJC Vaas|                 16.0|           13|0.93396|1.40094|\n",
      "|  15|   DE Bollinger|             16.21622|           27|0.92925|1.39387|\n",
      "|  16|NM Coulter-Nile|             16.27778|           25|0.92453|1.38679|\n",
      "|  17| AD Mascarenhas|             16.31579|           13|0.91981|1.37972|\n",
      "|  18|    MF Maharoof|             16.33333|           20|0.91509|1.37264|\n",
      "|  19|       MR Marsh|                16.35|           18|0.91038|1.36557|\n",
      "|  20|    AB McDonald|             17.09091|           10|0.90566|1.35849|\n",
      "+----+---------------+---------------------+-------------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wta = spark.sql('select *,round((212-Rank)/212, 5) as Points,round(1.5*(212-Rank)/212, 5) as Weight \\\n",
    "                from wicket_taking_ability')\n",
    "wta.registerTempTable('wicket_taking_ability')\n",
    "wta.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|        Bowler|Consistency|\n",
      "+--------------+-----------+\n",
      "|         A Roy|       14.0|\n",
      "| Kuldeep Yadav|   30.07692|\n",
      "|    TM Dilshan|       73.6|\n",
      "|       J Botha|      32.72|\n",
      "|    KA Pollard|     31.875|\n",
      "| LA Carseldine|        6.0|\n",
      "|M Muralitharan|   27.57813|\n",
      "|      DR Smith|   31.73077|\n",
      "|Jaskaran Singh|       29.0|\n",
      "|    A Flintoff|       53.0|\n",
      "+--------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consistency = Runs Conceded/Wickets Taken\n",
    "cons = spark.sql('select t1.bowler as Bowler, round(t1.runs/t2.wickets, 5) as Consistency \\\n",
    "                        from (select sum(total_runs) as runs,bowler from analysis_db group by bowler) t1 \\\n",
    "                        inner join \\\n",
    "                        (Select bowler,count(*) as wickets from analysis_db where player_dismissed is not null \\\n",
    "                        and (dismissal_kind = \\'bowled\\' or  dismissal_kind = \\'hit wicket\\' \\\n",
    "                        or  dismissal_kind = \\'stumped\\' or  dismissal_kind = \\'lbw\\' \\\n",
    "                        or  dismissal_kind = \\'caught and bowled\\' or  dismissal_kind = \\'caught\\') \\\n",
    "                        group by bowler)t2 on t1.bowler = t2.bowler')\n",
    "cons.registerTempTable('consistency')\n",
    "cons.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+-----------+-------------+\n",
      "|Rank|        Bowler|Consistency|No_of_Matches|\n",
      "+----+--------------+-----------+-------------+\n",
      "|   1| Sohail Tanvir|       12.5|           11|\n",
      "|   2|       A Zampa|   14.78947|           11|\n",
      "|   3|        N Rana|   17.71429|           12|\n",
      "|   4|  CRD Fernando|       18.0|           10|\n",
      "|   5|      BJ Hodge|   18.23529|           20|\n",
      "|   6|       K Ahmed|   18.47368|           10|\n",
      "|   7|AD Mascarenhas|   19.21053|           13|\n",
      "|   8|      K Rabada|   19.32258|           18|\n",
      "|   9|  DE Bollinger|   19.35135|           27|\n",
      "|  10|   MF Maharoof|    19.7037|           20|\n",
      "+----+--------------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cons = spark.sql('select row_number() over (order by c.Consistency asc) as Rank, c.*,n.No_of_Matches \\\n",
    "                    from consistency c \\\n",
    "                    inner join \\\n",
    "                    no_of_matches_bowlers n on \\\n",
    "                    c.Bowler = n.Bowler where n.No_of_Matches > 9')\n",
    "cons.registerTempTable('consistency')\n",
    "cons.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+-----------+-------------+-------+-------+\n",
      "|Rank|        Bowler|Consistency|No_of_Matches| Points|Weights|\n",
      "+----+--------------+-----------+-------------+-------+-------+\n",
      "|   1| Sohail Tanvir|       12.5|           11|0.99528|0.99528|\n",
      "|   2|       A Zampa|   14.78947|           11|0.99057|0.99057|\n",
      "|   3|        N Rana|   17.71429|           12|0.98585|0.98585|\n",
      "|   4|  CRD Fernando|       18.0|           10|0.98113|0.98113|\n",
      "|   5|      BJ Hodge|   18.23529|           20|0.97642|0.97642|\n",
      "|   6|       K Ahmed|   18.47368|           10| 0.9717| 0.9717|\n",
      "|   7|AD Mascarenhas|   19.21053|           13|0.96698|0.96698|\n",
      "|   8|      K Rabada|   19.32258|           18|0.96226|0.96226|\n",
      "|   9|  DE Bollinger|   19.35135|           27|0.95755|0.95755|\n",
      "|  10|   MF Maharoof|    19.7037|           20|0.95283|0.95283|\n",
      "+----+--------------+-----------+-------------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cons = spark.sql('select *,round((212-Rank)/212, 5) as Points, round((212-Rank)/212, 5) as Weights from consistency')\n",
    "cons.registerTempTable('consistency')\n",
    "cons.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crucial Wicket Taking Ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------------+\n",
      "|         Bowler|Crucial_Wicket_Taking_Ablity|\n",
      "+---------------+----------------------------+\n",
      "|       A Joseph|                     0.33333|\n",
      "|  Shoaib Akhtar|                     0.33333|\n",
      "|  Sohail Tanvir|                     0.18182|\n",
      "|     YA Abdulla|                     0.18182|\n",
      "|       Umar Gul|                     0.16667|\n",
      "|         AJ Tye|                     0.15385|\n",
      "|        L Ngidi|                     0.14286|\n",
      "|Karanveer Singh|                     0.11111|\n",
      "|       S Curran|                     0.11111|\n",
      "|       K Rabada|                     0.11111|\n",
      "+---------------+----------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crucial Wicket Taking Ability = Number of times Four or Five Wickets Taken/Number of Innings Played\n",
    "cwta = spark.sql('select t2.bowler as Bowler, round(nvl(t1.no_of_4wickets/t2.innings,0), 5) as Crucial_Wicket_Taking_Ablity \\\n",
    "                 from (select bowler,count(*) as no_of_4wickets from (select * from \\\n",
    "                 (select match_id,bowler,count(*) as wickets from analysis_db where player_dismissed \\\n",
    "                 is not null \\\n",
    "                 and (dismissal_kind = \\'bowled\\' or  dismissal_kind = \\'hit wicket\\' \\\n",
    "                 or  dismissal_kind = \\'stumped\\' or  dismissal_kind = \\'lbw\\' \\\n",
    "                 or  dismissal_kind = \\'caught and bowled\\' or  dismissal_kind = \\'caught\\') \\\n",
    "                 group by bowler,match_id ) \\\n",
    "                 where wickets > 3) group by bowler)t1 \\\n",
    "                 right join \\\n",
    "                 (select bowler,count(match_id) as \\\n",
    "                 innings from (select distinct(match_id),bowler from analysis_db) \\\n",
    "                 group by bowler)t2 \\\n",
    "                 on t1.bowler = t2.bowler order by Crucial_Wicket_Taking_Ablity desc')\n",
    "cwta.registerTempTable('crucial_wicket_taking_ablity')\n",
    "cwta.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+----------------------------+-------------+\n",
      "|Rank|       Bowler|Crucial_Wicket_Taking_Ablity|No_of_Matches|\n",
      "+----+-------------+----------------------------+-------------+\n",
      "|   1|Sohail Tanvir|                     0.18182|           11|\n",
      "|   1|   YA Abdulla|                     0.18182|           11|\n",
      "|   3|       AJ Tye|                     0.15385|           26|\n",
      "|   4|     K Rabada|                     0.11111|           18|\n",
      "|   5|     J Theron|                         0.1|           10|\n",
      "|   5|  PC Valthaty|                         0.1|           10|\n",
      "|   5| CRD Fernando|                         0.1|           10|\n",
      "|   8|      A Zampa|                     0.09091|           11|\n",
      "|   8|    CJ Jordan|                     0.09091|           11|\n",
      "|  10|   A Chandila|                     0.08333|           12|\n",
      "+----+-------------+----------------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cwta = spark.sql('select rank() over (order by cw.Crucial_Wicket_Taking_Ablity desc) as Rank, cw.*,n.No_of_Matches \\\n",
    "                 from crucial_wicket_taking_ablity cw \\\n",
    "                 inner join no_of_matches_bowlers n on \\\n",
    "                 cw.Bowler = n.Bowler where n.No_of_Matches > 9')\n",
    "cwta.registerTempTable('crucial_wicket_taking_ablity')\n",
    "cwta.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+----------------------------+-------------+-------+-------+\n",
      "|Rank|       Bowler|Crucial_Wicket_Taking_Ablity|No_of_Matches| Points|Weights|\n",
      "+----+-------------+----------------------------+-------------+-------+-------+\n",
      "|   1|Sohail Tanvir|                     0.18182|           11|0.99528|1.49293|\n",
      "|   1|   YA Abdulla|                     0.18182|           11|0.99528|1.49293|\n",
      "|   3|       AJ Tye|                     0.15385|           26|0.98585|1.47877|\n",
      "|   4|     K Rabada|                     0.11111|           18|0.98113|1.47170|\n",
      "|   5|     J Theron|                         0.1|           10|0.97642|1.46462|\n",
      "|   5|  PC Valthaty|                         0.1|           10|0.97642|1.46462|\n",
      "|   5| CRD Fernando|                         0.1|           10|0.97642|1.46462|\n",
      "|   8|      A Zampa|                     0.09091|           11|0.96226|1.44340|\n",
      "|   8|    CJ Jordan|                     0.09091|           11|0.96226|1.44340|\n",
      "|  10|   A Chandila|                     0.08333|           12|0.95283|1.42925|\n",
      "+----+-------------+----------------------------+-------------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cwta = spark.sql('select *,round((212-Rank)/212, 5) as Points, round(1.5*(212-Rank)/212, 5) as Weights \\\n",
    "                 from crucial_wicket_taking_ablity')\n",
    "cwta.registerTempTable('crucial_wicket_taking_ablity')\n",
    "cwta.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Performance Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------------+\n",
      "|         Bowler|Short_Performance_Index|\n",
      "+---------------+-----------------------+\n",
      "|     SL Malinga|                1.22609|\n",
      "|        B Kumar|                1.05263|\n",
      "|       MM Patel|                1.01667|\n",
      "|         AJ Tye|                    1.0|\n",
      "|       A Mishra|                0.97203|\n",
      "|      SP Narine|                0.91176|\n",
      "|Harbhajan Singh|                0.90968|\n",
      "|       L Balaji|                0.85507|\n",
      "|    JP Faulkner|                0.82456|\n",
      "|       A Kumble|                0.82051|\n",
      "+---------------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Short Performance Index = (Wickets Taken – 4* Number of Times Four Wickets Taken – 5* Number of Times Five Wickets Taken)/(Innings Played – Number of Times Four Wickets or Five Wickets Taken)\n",
    "spi = spark.sql('select n.bowler as Bowler, round(nvl(t5.Short_Performance_Index,0), 5) as Short_Performance_Index \\\n",
    "                from (select t1.bowler,(t3.wickets - 4*t1.no_of_4wickets - 5*t2.no_of_4wickets)/ \\\n",
    "                (t4.innings - t1.no_of_4wickets - t2.no_of_4wickets) as Short_Performance_Index \\\n",
    "                from (select bowler,count(*) as no_of_4wickets \\\n",
    "                from (select * from (select match_id,bowler,count(*) as wickets from analysis_db where player_dismissed \\\n",
    "                is not null \\\n",
    "                and (dismissal_kind = \\'bowled\\' or  dismissal_kind = \\'hit wicket\\' \\\n",
    "                or  dismissal_kind = \\'stumped\\' or  dismissal_kind = \\'lbw\\' \\\n",
    "                or  dismissal_kind = \\'caught and bowled\\' or  dismissal_kind = \\'caught\\')\\\n",
    "                group by bowler, match_id ) \\\n",
    "                where wickets = 4) group by bowler) t1 \\\n",
    "                inner join \\\n",
    "                (select bowler,count(*) as no_of_4wickets from (select * from \\\n",
    "                (select match_id,bowler,count(*) as wickets from analysis_db where player_dismissed \\\n",
    "                is not null \\\n",
    "                and (dismissal_kind = \\'bowled\\' or  dismissal_kind = \\'hit wicket\\' \\\n",
    "                or dismissal_kind = \\'stumped\\' or  dismissal_kind = \\'lbw\\' \\\n",
    "                or dismissal_kind = \\'caught and bowled\\' or  dismissal_kind = \\'caught\\')\\\n",
    "                group by bowler,match_id ) \\\n",
    "                where wickets = 5) group by bowler) t2 \\\n",
    "                inner join \\\n",
    "                (select bowler,count(*) as wickets from analysis_db where player_dismissed is not null \\\n",
    "                and (dismissal_kind = \\'bowled\\' or  dismissal_kind = \\'hit wicket\\' \\\n",
    "                or  dismissal_kind = \\'stumped\\' or  dismissal_kind = \\'lbw\\' \\\n",
    "                or  dismissal_kind = \\'caught and bowled\\' or  dismissal_kind = \\'caught\\') \\\n",
    "                group by bowler) t3 \\\n",
    "                inner join \\\n",
    "                (select bowler,count(match_id) as \\\n",
    "                innings from (select distinct(match_id),bowler from analysis_db) group by bowler) t4 \\\n",
    "                on t1.bowler = t2.bowler and t1.bowler = t3.bowler and t1.bowler = t4.bowler) t5 \\\n",
    "                right join \\\n",
    "                no_of_matches_bowlers n on t5.Bowler = n.Bowler order by Short_Performance_Index desc')\n",
    "spi.registerTempTable('short_performance_index')\n",
    "spi.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+-----------------------+-------------+\n",
      "|Rank|         Bowler|Short_Performance_Index|No_of_Matches|\n",
      "+----+---------------+-----------------------+-------------+\n",
      "|   1|     SL Malinga|                1.22609|          122|\n",
      "|   2|        B Kumar|                1.05263|          117|\n",
      "|   3|       MM Patel|                1.01667|           63|\n",
      "|   4|         AJ Tye|                    1.0|           26|\n",
      "|   5|       A Mishra|                0.97203|          147|\n",
      "|   6|      SP Narine|                0.91176|          109|\n",
      "|   7|Harbhajan Singh|                0.90968|          157|\n",
      "|   8|       L Balaji|                0.85507|           73|\n",
      "|   9|    JP Faulkner|                0.82456|           60|\n",
      "|  10|       A Kumble|                0.82051|           42|\n",
      "+----+---------------+-----------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spi = spark.sql('select rank() over (order by sp.Short_Performance_Index desc) as Rank, sp.*, n.No_of_Matches \\\n",
    "                from short_performance_index sp \\\n",
    "                inner join \\\n",
    "                no_of_matches_bowlers n on \\\n",
    "                sp.Bowler = n.Bowler where n.No_of_Matches > 9')\n",
    "spi.registerTempTable('short_performance_index')\n",
    "spi.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+-----------------------+-------------+-------+-------+\n",
      "|Rank|         Bowler|Short_Performance_Index|No_of_Matches| Points|Weights|\n",
      "+----+---------------+-----------------------+-------------+-------+-------+\n",
      "|   1|     SL Malinga|                1.22609|          122|0.99528|0.99528|\n",
      "|   2|        B Kumar|                1.05263|          117|0.99057|0.99057|\n",
      "|   3|       MM Patel|                1.01667|           63|0.98585|0.98585|\n",
      "|   4|         AJ Tye|                    1.0|           26|0.98113|0.98113|\n",
      "|   5|       A Mishra|                0.97203|          147|0.97642|0.97642|\n",
      "|   6|      SP Narine|                0.91176|          109| 0.9717| 0.9717|\n",
      "|   7|Harbhajan Singh|                0.90968|          157|0.96698|0.96698|\n",
      "|   8|       L Balaji|                0.85507|           73|0.96226|0.96226|\n",
      "|   9|    JP Faulkner|                0.82456|           60|0.95755|0.95755|\n",
      "|  10|       A Kumble|                0.82051|           42|0.95283|0.95283|\n",
      "+----+---------------+-----------------------+-------------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spi = spark.sql('select *,round((212-Rank)/212, 5) as Points, round((212-Rank)/212, 5) as Weights \\\n",
    "                from short_performance_index')\n",
    "spi.registerTempTable('short_performance_index')\n",
    "spi.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table Name for each Metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
